---
title: "Reviewing Advice"
author: Antoine Bosselut, Christophe Gravier, Jing Huang
author_profile: true
excerpt: ""
tags:
  Reviewer
  faqs
categories:
    blog
toc: true
toc_sticky: true
toc_icon: "cog"
---
##  Reviewing Advice for ACL-IJCNLP 2021
Adapted from the reviewing advice from NAACL-HLT 2021 and the reviewing advice from EMNLP 2020.

The intention for this page is to provide some advice to reviewers, such that we can identify the best research to be presented in the conference, and provide constructive feedback in order for authors to further improve their papers. We recognize and appreciate the amount of efforts reviewers have contributed, and hope to make that more beneficial. We want all the authors to feel the delight when they read the peer reviews for their papers.

To help our reviewers with common review questions, our Reviewer Mentoring Committee has created several videos and posted here. In addition, many major conferences have included advice to reviewers, in NLP and in other fields, and there's also plentiful advice relating to journal reviewing. Within the field of NLP, we would highlight:
- Discursive advice in ACL 2017 from leading lights in the field: Mirella Lapata, Marco Baroni, Yoav Artzi, Emily Bender, Joel Tetreault, Ani Nenkova, and Tim Baldwin
- Two example good reviews from NAACL 2018 presented in their reviewing form
- A podcast by Noah Smith about peer reviews

Please take the time to look through these excellent resources.

We hope reiterating some dos and don'ts here can help reviewers as well as authors.

**First, evaluate the paper's contributions.** This is where you will use your NLP domain knowledge. We advise that you should not accept papers just because their reported results are better, or that they appear to be mathematically sophisticated. These are not sufficient or necessary to constitute contributions. And we advise that you should not reject papers just because their results are not better than state-of-the-art. In the previous \*ACL conferences, some reviewers placed too much emphasis on SOTA performance, giving low scores to any systems that failed to reach that. While we aim to publish the very best work, a more constructive question to ask is **"state of which art?"**. As discussed in this blog post, a paper could offer a step forward in terms of efficiency, generalizability, interpretability, and many other criteria. A convincing contribution of any kind should not be rejected only for not topping the leaderboards.

Regarding different kinds of contributions, here's what Prof. Philip Resnik at University of Maryland says:

>I think there would be significant value in encouraging reviewers to think explicitly about the nature of the contribution, and what questions then need to be asked. As a first pass for consideration/discussion:
- Is this research making a **scientific** contribution? If so: 
	- What is the phenomenon in the world that the authors are seeking to improve our understanding of?
	- What do we now know about this phenomenon that we did not know before?
- Is this research making an **engineering** contribution? If so: 
	- What is the real-world problem (or set of problems) that this work is making progress on solving?
	- Alternatively, if it's not targeting a current real-world problem, what real-world problem(s) will this work help enable solutions of?
- Is this research making a **theoretical** (e.g. mathematical) contribution? If so: 
	- What do we know now that we did not know before?
	- How does this theoretical or mathematical advance connect to either scientific or engineering goals? (See above.)
<br><br>Work in computational linguistics might include a mixture of scientific, engineering, and theoretical contributions, rather than just one. But, I am suggesting, if a paper does not make a contribution in any of those three categories, with the sub-bullets having understandable answers, one should seriously consider whether it belongs at the conference.



